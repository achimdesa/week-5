{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Model Interpretability for NER\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, \n",
    "                          DistilBertTokenizerFast, DistilBertForTokenClassification,\n",
    "                          BertTokenizerFast, BertForTokenClassification)\n",
    "from shap import Explainer, summary_plot  # Import SHAP Explainer and summary_plot\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Step 1: Load the labeled dataset in CoNLL format\n",
    "def load_conll_data(file_path):\n",
    "    \"\"\"Loads CoNLL formatted data into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.strip().split()\n",
    "                sentence.append(token)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                if sentence:  # Check if sentence is not empty before appending\n",
    "                    data.append((\" \".join(sentence), labels))\n",
    "                    sentence = []\n",
    "                    labels = []\n",
    "    if sentence:  # For the last sentence if there is no newline\n",
    "        data.append((\" \".join(sentence), labels))\n",
    "\n",
    "    print(f\"Loaded {len(data)} sentences from the CoNLL file.\")  # Debugging info\n",
    "    return pd.DataFrame(data, columns=[\"text\", \"labels\"])\n",
    "\n",
    "# Load data\n",
    "conll_file_path = '../output/labeled_telegram_data.conll'  \n",
    "df = load_conll_data(conll_file_path)\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"xlm-roberta-base\"  # change the models name to when needed\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(f'../models/{model_name}')\n",
    "\n",
    "# Step 2: Prepare data for interpretation\n",
    "texts = df[\"text\"].tolist()[:2]  # Use the first two examples for demonstration\n",
    "\n",
    "# Tokenization\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Step 3: Make predictions with the fine-tuned model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract logits and predicted labels\n",
    "logits = outputs.logits\n",
    "predicted_labels = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Prepare the predictions for LIME and SHAP\n",
    "def get_predictions(text):\n",
    "    \"\"\"Get predictions from the model for LIME and SHAP.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return torch.argmax(outputs.logits, dim=2).tolist()\n",
    "\n",
    "# Step 4: Interpret model predictions using LIME\n",
    "lime_explainer = LimeTextExplainer(class_names=model.config.id2label.values())\n",
    "\n",
    "# Function to explain a single prediction\n",
    "def explain_with_lime(text):\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text,\n",
    "        get_predictions,\n",
    "        top_labels=1,\n",
    "        num_features=10\n",
    "    )\n",
    "    return explanation\n",
    "\n",
    "# Analyze LIME explanations\n",
    "for i in range(len(texts)):\n",
    "    explanation = explain_with_lime(texts[i])\n",
    "    print(f\"LIME Explanation for: {texts[i]}\")\n",
    "    explanation.as_pyplot_figure()  # Display the explanation plot\n",
    "\n",
    "# Step 5: Interpret model predictions using SHAP\n",
    "shap_explainer = Explainer(model)\n",
    "\n",
    "# Generate SHAP values for the first two texts\n",
    "shap_values = shap_explainer(inputs['input_ids'])\n",
    "\n",
    "# Plot SHAP values\n",
    "summary_plot(shap_values, inputs['input_ids'], feature_names=tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "print(\"Interpretation completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
