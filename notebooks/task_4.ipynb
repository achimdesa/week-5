{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, \n",
    "                          DistilBertTokenizerFast, DistilBertForTokenClassification,\n",
    "                          BertTokenizerFast, BertForTokenClassification,\n",
    "                          Trainer, TrainingArguments, DataCollatorForTokenClassification)\n",
    "import evaluate\n",
    "\n",
    "# Load the labeled dataset in CoNLL format\n",
    "def load_conll_data(file_path):\n",
    "    \"\"\"Loads CoNLL formatted data into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.strip().split()\n",
    "                sentence.append(token)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                data.append((sentence, labels))\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    if sentence:  # For the last sentence if there is no newline\n",
    "        data.append((sentence, labels))\n",
    "    return data\n",
    "\n",
    "conll_file_path = '../output/labeled_telegram_data.conll'\n",
    "data = load_conll_data(conll_file_path)\n",
    "\n",
    "# Convert data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['tokens', 'labels'])\n",
    "\n",
    "# Check the size of the dataset\n",
    "if len(df) > 1:\n",
    "    # Split into train and validation sets if you have more than one sample\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "else:\n",
    "    # If the dataset is too small, use the entire dataset for both training and validation\n",
    "    print(\"Dataset too small to split. Using the entire dataset for training and evaluation.\")\n",
    "    train_df = df\n",
    "    val_df = df\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenization and alignment of labels (same function as before)\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    \"\"\"Tokenizes inputs and aligns labels.\"\"\"\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # get word ids\n",
    "        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # default to -100 (ignore index)\n",
    "\n",
    "        # Align labels with tokenized inputs\n",
    "        for j, label_id in enumerate(label):\n",
    "            if j < len(word_ids) and word_ids[j] is not None:  # avoid IndexError\n",
    "                if label_id in label_map:  # Check if label_id exists in label_map\n",
    "                    label_ids[word_ids[j]] = label_map[label_id]  # map label to its id\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Map label names to IDs\n",
    "label_list = list(set(label for labels in df['labels'] for label in labels))\n",
    "label_list = sorted(label_list)\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Metrics function using Hugging Face's `evaluate` library\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    # Remove ignored index (-100)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Define model names for comparison\n",
    "model_names = [\n",
    "    \"xlm-roberta-base\",  # XLM-Roberta\n",
    "    \"distilroberta-base\",  # DistilRoBERTa\n",
    "    \"bert-base-multilingual-cased\",  # mBERT\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer and model based on the model name\n",
    "    try:\n",
    "        if model_name == \"xlm-roberta-base\":\n",
    "            tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "            model = XLMRobertaForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "        elif model_name == \"distilroberta-base\":\n",
    "            tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "            model = DistilBertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "        elif model_name == \"bert-base-multilingual-cased\":\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "            model = BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "        # Initialize the data collator (moved inside the loop)\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), \n",
    "                                                batched=True, remove_columns=['tokens', 'labels'])\n",
    "    tokenized_val_dataset = val_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), \n",
    "                                             batched=True, remove_columns=['tokens', 'labels'])\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'../results/{model_name}',  # output directory\n",
    "        evaluation_strategy=\"epoch\",           # evaluate every epoch\n",
    "        learning_rate=2e-5,                    # learning rate\n",
    "        per_device_train_batch_size=16,        # batch size for training\n",
    "        per_device_eval_batch_size=16,         # batch size for evaluation\n",
    "        num_train_epochs=3,                    # total number of training epochs\n",
    "        weight_decay=0.01,                     # strength of weight decay\n",
    "        logging_dir=f'../logs/{model_name}',   # directory for storing logs\n",
    "    )\n",
    "\n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,   # Use validation set for evaluation\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the fine-tuned model on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    results[model_name] = eval_results\n",
    "    print(f\"Evaluation results for {model_name}: {eval_results}\")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}: Precision={result['eval_precision']:.4f}, Recall={result['eval_recall']:.4f}, F1={result['eval_f1']:.4f}, Accuracy={result['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Step 7: Save the fine-tuned models\n",
    "for model_name in model_names:\n",
    "    model.save_pretrained(f'../models/{model_name}')\n",
    "    tokenizer.save_pretrained(f'../models/{model_name}')\n",
    "    print(f\"Model and tokenizer for {model_name} saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
