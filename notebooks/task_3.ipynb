{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Fine Tune NER Model for Amharic Telegram Messages\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Step 2: Load the labeled dataset in CoNLL format\n",
    "def load_conll_data(file_path):\n",
    "    \"\"\"Loads CoNLL formatted data into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.strip().split()\n",
    "                sentence.append(token)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                data.append((sentence, labels))\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    if sentence:  # For the last sentence if there is no newline\n",
    "        data.append((sentence, labels))\n",
    "    return data\n",
    "\n",
    "# Path to your labeled data in CoNLL format\n",
    "conll_file_path = '../output/labeled_telegram_data.conll'\n",
    "data = load_conll_data(conll_file_path)\n",
    "\n",
    "# Convert data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['tokens', 'labels'])\n",
    "\n",
    "# Step 3: Tokenize the data and align the labels with tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenizes inputs and aligns labels.\"\"\"\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # get word ids\n",
    "        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # default to -100 (ignore index)\n",
    "\n",
    "        # Align labels with tokenized inputs\n",
    "        for j, label_id in enumerate(label):\n",
    "            # Check if word_ids[j] exists to prevent IndexError\n",
    "            if j < len(word_ids) and word_ids[j] is not None:  # only consider non-padding tokens\n",
    "                if label_id in label_map:  # Check if label_id exists in label_map\n",
    "                    label_ids[word_ids[j]] = label_map[label_id]  # map label to its id\n",
    "\n",
    "        # Append the aligned labels\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"xlm-roberta-base\"  # can be replaced \"bert-tiny-amharic\" or \"afroxmlr\"\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Map label names to IDs\n",
    "label_list = list(set(label for labels in df['labels'] for label in labels))\n",
    "label_list = sorted(label_list)\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=['tokens', 'labels'])\n",
    "\n",
    "# Step 4: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results',            # output directory\n",
    "    evaluation_strategy=\"epoch\",       # evaluate every epoch\n",
    "    learning_rate=2e-5,                # learning rate\n",
    "    per_device_train_batch_size=16,    # batch size for training\n",
    "    per_device_eval_batch_size=16,     # batch size for evaluation\n",
    "    num_train_epochs=3,                 # total number of training epochs\n",
    "    weight_decay=0.01,                  # strength of weight decay\n",
    ")\n",
    "\n",
    "# Step 5: Use Hugging Face's Trainer API to fine-tune the model\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # You may want to create a validation set separately\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Step 6: Evaluate the fine-tuned model on the validation set\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# Step 7: Save the fine-tuned model\n",
    "model.save_pretrained('../models/fine_tuned_ner_model')\n",
    "tokenizer.save_pretrained('../models/fine_tuned_ner_model')\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
